{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f2310b-0960-4972-ac05-f8a91dc29918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zxc/.local/lib/python3.13/site-packages/torch/backends/__init__.py:46: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  self.setter(val)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple, Union, List\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "\n",
    "import matplotlib_inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "torch.autograd.profiler.profile(False)\n",
    "torch.autograd.profiler.emit_nvtx(False)\n",
    "torch.autograd.set_detect_anomaly(False)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d11d8f-77e9-4367-a21c-678cf84018d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"Set global seed for reproducibility.\n",
    "    :param int seed: Seed to be set\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def check_numel(module: torch.nn.Module, params_numel: int, buffers_numel: Optional[int] = None) -> None:\n",
    "    \"\"\"Check whether module has correct number of parameters and buffers\n",
    "    :param torch.nn.Module module: Target model\n",
    "    :param int params_numel: Target number of parameters\n",
    "    :param Optional[int] buffers_numel: Target number of buffers\n",
    "    :rtype:\n",
    "    \"\"\"\n",
    "    numel = sum(param.numel() for param in module.parameters())\n",
    "    assert numel == params_numel, f'For params numel != correct numel: {numel} vs {params_numel}'\n",
    "    \n",
    "    if buffers_numel is not None:\n",
    "        numel = sum(param.numel() for param in module.buffers())\n",
    "        assert numel == buffers_numel, f'For buffers numel != correct numel: {numel} vs {buffers_numel}'\n",
    "\n",
    "set_global_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46a0f7ea-2629-40e3-bf1a-e59a4be0cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_cifar(train, transform=None):    \n",
    "    if transform is None:\n",
    "        transform = T.ToTensor()\n",
    "    return torchvision.datasets.CIFAR10(\n",
    "        root='./data',\n",
    "        train=train,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04dff777-cdd3-4230-9d2f-11c1d14de708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n",
      "tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n",
      "         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n",
      "         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n",
      "         ...,\n",
      "         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n",
      "         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n",
      "         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n",
      "\n",
      "        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n",
      "         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n",
      "         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n",
      "         ...,\n",
      "         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n",
      "         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n",
      "         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n",
      "\n",
      "        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n",
      "         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n",
      "         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n",
      "         ...,\n",
      "         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n",
      "         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n",
      "         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])\n",
      "MEAN, STD: 0.4733630120754242 0.2515689432621002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "train_data = load_cifar(True, T.ToTensor())\n",
    "\n",
    "print(train_data[0][0].size()) # размеры тензора\n",
    "\n",
    "IMAGE_DIMENSION = 32\n",
    "CHANNEL_NUM = 3\n",
    "\n",
    "print(train_data[0][0])\n",
    "\n",
    "images = torch.cat([tensor for tensor, ans in train_data])\n",
    "answers = [ans for tensor, ans in train_data]\n",
    "\n",
    "mean = images.mean().item()\n",
    "std = images.std().item()\n",
    "print(\"MEAN, STD:\", mean, std)\n",
    "\n",
    "\n",
    "norm_transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "train_cifar = load_cifar(True, norm_transform)\n",
    "val_cifar = load_cifar(False, norm_transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4580534-5115-4555-a7df-30c6e720f70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_SIZE: 50000\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_TEST = 1000\n",
    "\n",
    "train_cifar_loader = DataLoader(\n",
    "    dataset=train_cifar,\n",
    "    batch_size=BATCH_SIZE_TRAIN,\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_cifar_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_cifar,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "DATASET_SIZE = len(train_data)\n",
    "print(\"DATASET_SIZE:\", DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "065293e0-0d14-4b4b-b8fa-06f2de35a1dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3, 32, 32]), torch.Size([64]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CIFAR10_LABELS_LIST = [\n",
    "    'airplane', \n",
    "    'automobile',\n",
    "    'bird',\n",
    "    'cat',\n",
    "    'deer',\n",
    "    'dog',\n",
    "    'frog',\n",
    "    'horse',\n",
    "    'ship',\n",
    "    'truck'\n",
    "]\n",
    "\n",
    "\n",
    "images, labels = next(iter(train_cifar_loader))\n",
    "images.shape, labels.shape #насколько я понял здесь мы сделали магию с указателями - почему то от начала надо взять next чтобы получить начальный элемент\n",
    "#просто проверяем что загрузилось корректно короче\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6d17ae7-911a-49aa-958c-84ba178316f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/pdf": "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",
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"280.512pt\" height=\"295.630125pt\" viewBox=\"0 0 280.512 295.630125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-11-07T23:53:24.495430</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.10.7, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M 0 295.630125 \n",
       "L 280.512 295.630125 \n",
       "L 280.512 0 \n",
       "L 0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g clip-path=\"url(#p9cafeec93f)\">\n",
       "    <image xlink:href=\"data:image/png;base64,\n",
       "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\" id=\"image947652148c\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"7.2\" y=\"-22.030125\" width=\"266.4\" height=\"266.4\"/>\n",
       "   </g>\n",
       "   <g id=\"text_1\">\n",
       "    <!-- object bird -->\n",
       "    <g transform=\"translate(108.21225 16.318125) scale(0.12 -0.12)\">\n",
       "     <defs>\n",
       "      <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-62\" d=\"M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "M 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2969 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-6a\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 -63 \n",
       "Q 1178 -731 923 -1031 \n",
       "Q 669 -1331 103 -1331 \n",
       "L -116 -1331 \n",
       "L -116 -844 \n",
       "L 38 -844 \n",
       "Q 366 -844 484 -692 \n",
       "Q 603 -541 603 -63 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      <path id=\"DejaVuSans-64\" d=\"M 2906 2969 \n",
       "L 2906 4863 \n",
       "L 3481 4863 \n",
       "L 3481 0 \n",
       "L 2906 0 \n",
       "L 2906 525 \n",
       "Q 2725 213 2448 61 \n",
       "Q 2172 -91 1784 -91 \n",
       "Q 1150 -91 751 415 \n",
       "Q 353 922 353 1747 \n",
       "Q 353 2572 751 3078 \n",
       "Q 1150 3584 1784 3584 \n",
       "Q 2172 3584 2448 3432 \n",
       "Q 2725 3281 2906 2969 \n",
       "z\n",
       "M 947 1747 \n",
       "Q 947 1113 1208 752 \n",
       "Q 1469 391 1925 391 \n",
       "Q 2381 391 2643 752 \n",
       "Q 2906 1113 2906 1747 \n",
       "Q 2906 2381 2643 2742 \n",
       "Q 2381 3103 1925 3103 \n",
       "Q 1469 3103 1208 2742 \n",
       "Q 947 2381 947 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "     </defs>\n",
       "     <use xlink:href=\"#DejaVuSans-6f\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(61.181641 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-6a\" transform=\"translate(124.658203 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-65\" transform=\"translate(152.441406 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-63\" transform=\"translate(213.964844 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-74\" transform=\"translate(268.945312 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-20\" transform=\"translate(308.154297 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-62\" transform=\"translate(339.941406 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-69\" transform=\"translate(403.417969 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-72\" transform=\"translate(431.201172 0)\"/>\n",
       "     <use xlink:href=\"#DejaVuSans-64\" transform=\"translate(470.564453 0)\"/>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p9cafeec93f\">\n",
       "   <rect x=\"7.2\" y=\"22.318125\" width=\"266.112\" height=\"266.112\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "denorm_transform = T.Compose([\n",
    "    T.Normalize(mean=[0, 0, 0], std=[1/std, 1/std, 1/std]),\n",
    "    T.Normalize(mean=[-mean, -mean, -mean], std=[1, 1, 1])\n",
    "])\n",
    "\n",
    "i = 3\n",
    "denormed_image = denorm_transform(images[i])\n",
    "\n",
    "# Отображаем\n",
    "plt.title(f'object {CIFAR10_LABELS_LIST[labels[i]]}')\n",
    "plt.imshow(denormed_image.permute(1, 2, 0).numpy()) \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b4acbd7-76f3-4a4f-b149-f14be4ae20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    network,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    LOGGING_INTERVAL\n",
    ") -> float:\n",
    "    \"\"\"train `network` with `optimizer` for one epoch with data from `train_loader` to minimize `criterion`\"\"\"\n",
    "    \n",
    "    network.train()  # switch network submodules to train mode, e.g. it influences on batch-norm, dropout\n",
    "    id = 0\n",
    "    TOTAL_DONE = 0\n",
    "    TOTAL_LOSS = 0\n",
    "    for images, answers in train_loader:\n",
    "        id += 1\n",
    "        # 0. распакавываем данные на нужное устройство\n",
    "        images, answers = images.to(device), answers.to(device) # чего? кажется этот код смещает нагрузку на видеокарту но у меня встройка и линукс так что оно в моем случае ниче не делает\n",
    "\n",
    "        # 1. сбрасываем градиенты предыдущего батча\n",
    "        optimizer.zero_grad() # YOUR CODE HERE\n",
    "\n",
    "        # 2. прогоняем данные через нашу нейросеть\n",
    "        logits = network(images)\n",
    "\n",
    "        # 3. считаем loss\n",
    "        loss = criterion(logits, answers) # кажется так надо передавать reduction\n",
    "        # 4. считаем градиенты\n",
    "        loss.backward() # YOUR CODE HERE\n",
    "        TOTAL_LOSS += loss.item()\n",
    "\n",
    "        # 5. обновляем параметры\n",
    "        optimizer.step() # YOUR CODE HERE\n",
    "\n",
    "        TOTAL_DONE += answers.size(0)\n",
    "\n",
    "        if (id % LOGGING_INTERVAL == 0):\n",
    "            loss_num = loss.item()\n",
    "            _, my_answers = torch.max(logits, 1)\n",
    "            num_predicted = (my_answers == answers).sum().item()\n",
    "            total = answers.size(0)\n",
    "\n",
    "            print(id, \"ITERATIONS\", \"[\", TOTAL_DONE, '/', DATASET_SIZE, '(', TOTAL_DONE / DATASET_SIZE * 100, '%)]\\t LOSS:', loss.item())\n",
    "    return TOTAL_LOSS / id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b109d3c4-1490-4d24-a770-675c3c7918b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#то же самое только тест\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch(\n",
    "    network,\n",
    "    val_loader,\n",
    "    criterion\n",
    "):\n",
    "    \"\"\"calculate loss and accuracy on validation data\"\"\"\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    TOTAL_DONE = 0\n",
    "\n",
    "    network.eval()  # switch network submodules to test mode\n",
    "    for images, answers in val_loader:\n",
    "        # 0. распакавываем данные на нужное устройство\n",
    "        images, answers = images.to(device), answers.to(device) # YOUR CODE HERE\n",
    "\n",
    "        # 1. прогоняем данные через нашу нейросеть\n",
    "        logits = network(images) # YOUR CODE HERE\n",
    "\n",
    "         # 2. получаем предсказание\n",
    "        pred = torch.argmax(logits, dim=1) # YOUR CODE HERE\n",
    "\n",
    "        # 3. логгируем лосс и accuracy\n",
    "        val_loss += criterion(logits, answers).item()\n",
    "        correct += torch.sum(pred == answers).item()\n",
    "\n",
    "        TOTAL_DONE += answers.size(0)\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accur = correct / len(val_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f'Test set: Avg. loss: {val_loss:.4f}',\n",
    "        f'Accuracy: {correct}/{len(val_loader.dataset)}',\n",
    "        f'({100. * val_accur:.0f}%)',\n",
    "    )\n",
    "\n",
    "    return val_loss, val_accur\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19791667-2209-491a-af38-91bf7100a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def train_val(\n",
    "    network,\n",
    "    n_epochs,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    LOGGING_INTERVAL\n",
    "):\n",
    "    \"\"\"full cycle of neural network training\"\"\"\n",
    "\n",
    "    writer = SummaryWriter('runs/cifar')\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "    best_acc = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accs = []\n",
    "\n",
    "\n",
    "    val_epoch(network, val_loader, criterion)\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        avg_train_loss = train_epoch(network, train_loader, criterion, optimizer, LOGGING_INTERVAL)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        val_loss, val_accuracy = val_epoch(network, val_loader, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/val', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/val', val_accuracy, epoch)\n",
    "\n",
    "\n",
    "        if (val_accuracy > best_acc):\n",
    "            best_acc = val_accuracy\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': network.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_loss': val_loss,\n",
    "            }, 'models/best.pth')\n",
    "\n",
    "            print(\"NEW RECORD\", best_acc * 100, '%')\n",
    "\n",
    "            \n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "    checkpoint = torch.load('models/best.pth')\n",
    "    network.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return train_losses, val_losses, val_accs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa1f0642-5324-454a-b464-d2f8bfaf7c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LogReg(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.flatten = nn.Flatten() # [X, 1, 28, 28] -> [X, 784] - преобразуемым данные в формат для линейной комбинации\n",
    "        self.linear = nn.Linear(IMAGE_DIMENSION * IMAGE_DIMENSION * CHANNEL_NUM, len(CIFAR10_LABELS_LIST))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "\n",
    "        return self.linear(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e00ed216-5f10-4f4c-9210-ea212d9bfd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN TRAINING\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# создали нейронку\n",
    "net = LogReg()\n",
    "\n",
    "\n",
    "#тренируем\n",
    "\n",
    "#выход нейронки - p_i = \\sigma(out_i)\n",
    "\n",
    "net = LogReg().to(device) # на GPU\n",
    "criterion = nn.CrossEntropyLoss() # [LOSS_i = \\sum_{i=0}^9{ln n_i * p_i - (1-n_i) * (1 - p_i)} - n_i = 1 если мы зашадали iую цифру\n",
    "# берем такую функцию loss насколько я понял, чтобы там хорошие производные получались - ведь мы буквально берем производную по Loss чтобы найти где минимум ошибки\n",
    "#out_i = bias_i + \\sum{w_i * x_i} - w_i - вес, x_i - значение\n",
    "#таким образом производные получатся:\n",
    "\n",
    "#здесь будет вывод мб, пока не написал\n",
    "\n",
    "# Оптимизатор - Stochastic Gradient Descent\n",
    "\n",
    "print(\"BEGIN TRAINING\")\n",
    "\n",
    "#train_val(net, 10, criterion, train_cifar_loader, val_cifar_loader, 300)\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44896ebf-f550-4473-95a4-270d14d2d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31f6d341-5f3c-4d04-bae9-2e15b572841c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6a06e9ab85a0bcc1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6a06e9ab85a0bcc1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir .\n",
    "#30 процентов - как то грустно - я загуглил какие другие простые модели существует - выдало что то про CNN, попробую реализовать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71e0bade-7400-4c6c-bf5b-6e9e02faf6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.drop = nn.Dropout(0.25)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.drop(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b4748552-22ea-42e3-9290-6e78b65a8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEGIN CNN TRAINING\n",
      "Test set: Avg. loss: 0.0023 Accuracy: 961/10000 (10%)\n",
      "100 ITERATIONS [ 6400 / 50000 ( 12.8 %)]\t LOSS: 1.772432565689087\n",
      "200 ITERATIONS [ 12800 / 50000 ( 25.6 %)]\t LOSS: 1.3086721897125244\n",
      "300 ITERATIONS [ 19200 / 50000 ( 38.4 %)]\t LOSS: 1.188489556312561\n",
      "400 ITERATIONS [ 25600 / 50000 ( 51.2 %)]\t LOSS: 1.120774269104004\n",
      "500 ITERATIONS [ 32000 / 50000 ( 64.0 %)]\t LOSS: 1.0719904899597168\n",
      "600 ITERATIONS [ 38400 / 50000 ( 76.8 %)]\t LOSS: 1.1499156951904297\n",
      "700 ITERATIONS [ 44800 / 50000 ( 89.60000000000001 %)]\t LOSS: 1.1077417135238647\n",
      "Test set: Avg. loss: 0.0010 Accuracy: 6353/10000 (64%)\n",
      "NEW RECORD 63.53 %\n",
      "100 ITERATIONS [ 6400 / 50000 ( 12.8 %)]\t LOSS: 0.8492021560668945\n",
      "200 ITERATIONS [ 12800 / 50000 ( 25.6 %)]\t LOSS: 0.920577883720398\n",
      "300 ITERATIONS [ 19200 / 50000 ( 38.4 %)]\t LOSS: 0.7681818604469299\n",
      "400 ITERATIONS [ 25600 / 50000 ( 51.2 %)]\t LOSS: 0.9012620449066162\n",
      "500 ITERATIONS [ 32000 / 50000 ( 64.0 %)]\t LOSS: 1.2060264348983765\n",
      "600 ITERATIONS [ 38400 / 50000 ( 76.8 %)]\t LOSS: 0.8715479969978333\n",
      "700 ITERATIONS [ 44800 / 50000 ( 89.60000000000001 %)]\t LOSS: 1.062972903251648\n",
      "Test set: Avg. loss: 0.0009 Accuracy: 6890/10000 (69%)\n",
      "NEW RECORD 68.89999999999999 %\n",
      "100 ITERATIONS [ 6400 / 50000 ( 12.8 %)]\t LOSS: 0.994354248046875\n",
      "200 ITERATIONS [ 12800 / 50000 ( 25.6 %)]\t LOSS: 0.7410300970077515\n",
      "300 ITERATIONS [ 19200 / 50000 ( 38.4 %)]\t LOSS: 1.1055058240890503\n",
      "400 ITERATIONS [ 25600 / 50000 ( 51.2 %)]\t LOSS: 0.66725093126297\n",
      "500 ITERATIONS [ 32000 / 50000 ( 64.0 %)]\t LOSS: 1.018361210823059\n",
      "600 ITERATIONS [ 38400 / 50000 ( 76.8 %)]\t LOSS: 0.6310276389122009\n",
      "700 ITERATIONS [ 44800 / 50000 ( 89.60000000000001 %)]\t LOSS: 0.6655400395393372\n",
      "Test set: Avg. loss: 0.0008 Accuracy: 6991/10000 (70%)\n",
      "NEW RECORD 69.91000000000001 %\n",
      "100 ITERATIONS [ 6400 / 50000 ( 12.8 %)]\t LOSS: 0.6855059266090393\n",
      "200 ITERATIONS [ 12800 / 50000 ( 25.6 %)]\t LOSS: 0.782323956489563\n",
      "300 ITERATIONS [ 19200 / 50000 ( 38.4 %)]\t LOSS: 0.7249472737312317\n",
      "400 ITERATIONS [ 25600 / 50000 ( 51.2 %)]\t LOSS: 0.5916264653205872\n",
      "500 ITERATIONS [ 32000 / 50000 ( 64.0 %)]\t LOSS: 0.5869470834732056\n",
      "600 ITERATIONS [ 38400 / 50000 ( 76.8 %)]\t LOSS: 0.9366878867149353\n",
      "700 ITERATIONS [ 44800 / 50000 ( 89.60000000000001 %)]\t LOSS: 0.602367103099823\n",
      "Test set: Avg. loss: 0.0008 Accuracy: 7255/10000 (73%)\n",
      "NEW RECORD 72.55 %\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCNN training finished\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, train_losses, val_losses, val_accs\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m cnn_model, train_loss, val_loss, val_acc = \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mtrain_cnn\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m criterion = nn.CrossEntropyLoss()\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBEGIN CNN TRAINING\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m train_losses, val_losses, val_accs = \u001b[43mtrain_val\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_cifar_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_cifar_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCNN training finished\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model, train_losses, val_losses, val_accs\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_val\u001b[39m\u001b[34m(network, n_epochs, optimizer, criterion, train_loader, val_loader, LOGGING_INTERVAL)\u001b[39m\n\u001b[32m     24\u001b[39m val_epoch(network, val_loader, criterion)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, n_epochs + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     avg_train_loss = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLOGGING_INTERVAL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     train_losses.append(avg_train_loss)\n\u001b[32m     29\u001b[39m     val_loss, val_accuracy = val_epoch(network, val_loader, criterion)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(network, train_loader, criterion, optimizer, LOGGING_INTERVAL)\u001b[39m\n\u001b[32m     20\u001b[39m optimizer.zero_grad() \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# 2. прогоняем данные через нашу нейросеть\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m logits = \u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 3. считаем loss\u001b[39;00m\n\u001b[32m     26\u001b[39m loss = criterion(logits, answers) \u001b[38;5;66;03m# кажется так надо передавать reduction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mCNN_net.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     18\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop(x)\n\u001b[32m     19\u001b[39m x = torch.flatten(x, \u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m x = F.relu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     21\u001b[39m x = \u001b[38;5;28mself\u001b[39m.drop(x)\n\u001b[32m     22\u001b[39m x = \u001b[38;5;28mself\u001b[39m.fc2(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.13/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def train_cnn():\n",
    "    model = CNN_net().to(device)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    print(\"BEGIN CNN TRAINING\")\n",
    "    \n",
    "    train_losses, val_losses, val_accs = train_val(\n",
    "        model, 50, optimizer, criterion, train_cifar_loader, val_cifar_loader, 100\n",
    "    )\n",
    "    \n",
    "    print(\"CNN training finished\")\n",
    "    return model, train_losses, val_losses, val_accs\n",
    "\n",
    "cnn_model, train_loss, val_loss, val_acc = train_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df50ffc9-2069-46c1-b50c-ed1fbc20aeac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a25c9-9126-4275-bbb4-fad349147656",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32537a20-0a59-403d-93b0-0399e809bca8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99beee2-85f1-4341-82ea-92232e9c9e35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1654f8-b0ec-4d2e-bbd2-11e2fc3dc908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171660ab-a3a5-427e-9b58-ef223a899f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
